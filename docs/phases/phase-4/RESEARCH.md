# Research: Phase 4

## Phase Context
Phase 4 requires building a static Quarto executive report that generates polished HTML/PDF summaries with top performers, score distributions, and trend analysis. Additionally, implement an improvement suggestions engine using rule-based logic to identify coaching opportunities based on dimension score patterns. The report should be shareable for leadership, and suggestions should provide actionable recommendations for managers.

## Previous Phase Learnings

### Key Points from Phase 3 REFLECTIONS.md

**Dependency Configuration Issues (REFLECTIONS.md:59-63, 99-128)**
- `plotly` declared in `Suggests` but used as hard dependency in `app.R:15` — should be in `Imports`
- `ggplot2` used via namespace (`ggplot2::ggplot()`) but never explicitly loaded — fragile dependency relying on plotly loading ggplot2
- **Impact for Phase 4:** Must declare knitr, rmarkdown correctly in DESCRIPTION before implementation
- **Action:** Move new Phase 4 packages (knitr, rmarkdown) to Imports section immediately, validate before writing code

**Test-Driven Development Approach Worked Well (REFLECTIONS.md:7-12, 31-35)**
- Helper functions tested before UI integration caught real bugs (all-zero weights, missing non-dataframe check)
- 100% coverage maintained throughout Phase 3 via TDD
- **Impact for Phase 4:** Write tests for suggestion rules BEFORE implementing each rule
- **Action:** Create failing test for each suggestion pattern, then implement to make it pass

**Front-load Documentation During Implementation (REFLECTIONS.md:145-149)**
- Phase 3 updated all docs in final task (Task 10) causing documentation drift
- Better approach: Document features when implemented and tested
- **Impact for Phase 4:** Update AGENTS.md with Quarto installation instructions BEFORE writing report template
- **Action:** Document report generation workflow and suggestion rules in roxygen comments AS implementation progresses

**Manual Validation Required for UX-Critical Features (REFLECTIONS.md:151-154)**
- Charts, color coding, layout hard to validate via automated tests
- **Impact for Phase 4:** Report is user-facing deliverable for executives — must manually review for professionalism
- **Action:** Render sample report, open in browser, verify formatting/clarity before marking complete

**STATUS.md Update Discipline (REFLECTIONS.md:64-68, 246-249)**
- Phase 3 didn't update STATUS.md to completion state
- **Impact for Phase 4:** Track progress at major milestones
- **Action:** Update STATUS.md when suggestions engine complete, when report template complete, when phase complete

**shinytest2 Tests Skip in CI Environment (REFLECTIONS.md:52-57, 218-222)**
- All 8 E2E tests exist but skip with "On CRAN" reason
- Not a blocker for Phase 4 (report generation doesn't require Shiny tests)
- Dashboard integration of suggestions is optional/deferred — report is higher priority

## Current Codebase State

### Relevant Components

**Scoring Engine (Complete, Phase 2)** — `R/calculate_scores.R`
- Main entry point: `calculate_scores(data, weights, debug)` — `R/calculate_scores.R:60-97`
- Returns data frame with 4 score columns: `activity_score`, `conversion_score`, `revenue_score`, `score` (all 0-100)
- Accepts debug parameter to preserve intermediate columns (tenure_factor, territory_factor, quota_attainment) — `R/calculate_scores.R:46-48`
- No changes needed for Phase 4 — engine is complete and tested

**Dimension Scoring Functions** — `R/dimension_scoring.R`
- `score_activity()` — `R/dimension_scoring.R:18-37` — Combines calls, followups, meetings (normalized by tenure/territory)
- `score_conversion()` — `R/dimension_scoring.R:57-76` — Meetings-to-deals ratio + revenue per activity
- `score_revenue()` — `R/dimension_scoring.R:95-109` — Quota attainment + revenue per deal
- All functions use percentile ranking via `percentile_rank()` from `R/scoring_utils.R:57-66`
- Phase 4 suggestions engine will use these dimension scores as input for rule matching

**Normalization Functions** — `R/normalization.R`
- `normalize_tenure()` — `R/normalization.R:14-22` — tenure_factor = min(1.0, tenure_months / 60)
- `normalize_territory()` — `R/normalization.R:38-46` — territory_factor = territory_size / 100
- `normalize_quota()` — `R/normalization.R:61-74` — quota_attainment = (revenue / quota) * 100
- Not directly needed for Phase 4, but intermediate columns may be useful for debugging suggestions

**Scored Data Output** — `data/scored_reps.csv`
- Generated by `scripts/score_data.R`
- 80 rows (20 reps x 4 quarters): REP001-REP020, periods Q1-Q4 2025
- Columns (15 total): Original 11 columns + 4 score columns (activity_score, conversion_score, revenue_score, score)
- Example row: REP001, Rep A, tenure=10, score=49.16, activity=39.24, conversion=51.90, revenue=56.33
- **This is the primary input for Phase 4 report generation**

**Sample Data Generation** — `scripts/generate_data.R` + `R/generate_sample_data.R`
- Creates `data/sample_reps.csv` with 20 reps x 4 quarters
- Rep mix: 30% new (1-12 months tenure), 40% mid-level (13-36 months), 30% experienced (37-120 months)
- Territory sizes: 50-500 accounts, quotas: 50k-500k
- Phase 4 doesn't modify this — uses existing scored output

**Shiny Dashboard (Complete, Phase 3)** — `app.R`
- Single-file app structure (no modules) — `app.R:1-9` comments explain deferral decision
- Sources all scoring functions: `app.R:18-23`
- CSV export functionality: downloadHandler at `app.R:268-285` writes scored data with timestamp
- Filter controls: Rep and Period dropdowns with "Clear Filters" button — `app.R:47-56`
- Debug mode checkbox: `app.R:59-60` passes to `calculate_scores(debug = TRUE)`
- **Optional Phase 4 enhancement:** Add suggestions column to rankings table (deferred if time constrained)

**Test Infrastructure** — `tests/testthat/`
- Test framework: testthat 3rd edition — `DESCRIPTION:23`
- Test runner: `tests/testthat.R` standard entry point
- Current coverage: 100% for all R files (verified via `scripts/coverage_report.R`)
- Test pattern: Source files explicitly via `rprojroot::find_root("DESCRIPTION")` — see `tests/testthat/test-calculate_scores.R:2-5`
- 190 passing tests, 8 skipped (shinytest2 E2E tests skip with "On CRAN" reason)
- **Phase 4 test requirements:** Minimum 10 test cases for suggestions engine, maintain 100% coverage

**Helper Functions for Data Validation** — `R/shiny_helpers.R`
- `validate_upload_schema(data)` — `R/shiny_helpers.R:14-33` — Checks CSV has all 11 required columns
- Not directly needed for Phase 4, but pattern is useful: return list with `valid` (logical) and `message` (character)
- Phase 4 suggestions engine should use similar pattern for robust error handling

**Scoring Script Template** — `scripts/score_data.R`
- Sources all scoring functions: `scripts/score_data.R:6-10`
- Loads CSV, calls `calculate_scores()`, writes output: `scripts/score_data.R:12-40`
- **Pattern for Phase 4:** Create `scripts/generate_report.R` following same structure
- Should accept command-line arguments: --input, --output, --output-dir (per SPEC.md:60-68)

### Existing Patterns to Follow

**Function Documentation Pattern** — roxygen2 comments
- All R functions use roxygen2 format: `#'` prefix
- Required sections: `@param` (describe all parameters), `@return` (describe output), `@examples` (working code examples)
- Example: `R/calculate_scores.R:38-59` shows complete documentation for `calculate_scores()`
- **Phase 4 pattern:** Document `generate_suggestions()` and any helper functions with same structure

**Test File Naming Convention**
- Test files mirror source files: `test-<source_file>.R`
- Example: `R/calculate_scores.R` → `tests/testthat/test-calculate_scores.R`
- **Phase 4 pattern:** Create `tests/testthat/test-generate_suggestions.R` for suggestions engine

**Validation Helper Pattern** — `R/scoring_utils.R:17-23`
- Validation functions throw errors with descriptive messages using `stop()`
- Return `invisible(NULL)` if validation passes
- Example: `validate_columns()` checks required columns, stops with "Required columns missing: ..." message
- **Phase 4 pattern:** Use similar validation in suggestions engine for input data schema

**Percentile Ranking Pattern** — `R/scoring_utils.R:57-66`
- `percentile_rank(x)` converts numeric vector to 0-100 scale
- Handles edge cases: all-zero vectors return zeros, single value returns 0
- Uses `rank(x, ties.method = "average")` then scales to 0-100
- **Phase 4 usage:** Dimension scores already percentile-ranked, suggestions engine uses thresholds (e.g., > 75, < 50)

**Pipeline Function Pattern** — `R/calculate_scores.R:60-97`
- Accepts data frame input, validates, transforms, returns enhanced data frame
- Uses pipe operator `|>` for readability: `data |> normalize_tenure() |> normalize_territory() |> ...`
- Validates inputs before processing: `validate_columns()`, `validate_weights()` — `R/calculate_scores.R:67-68`
- **Phase 4 pattern:** `generate_suggestions()` should follow same structure (validate → transform → return data frame)

**Test Structure Pattern** — `tests/testthat/test-calculate_scores.R`
- Source all dependencies at top: `source(file.path(rprojroot::find_root("DESCRIPTION"), "R", "..."))`
- Group related tests with `test_that()` blocks
- Use descriptive test names: "validate_weights accepts valid weights", "validate_weights rejects negative weights"
- **Phase 4 pattern:** Create test_that blocks for each of 5 suggestion rules, plus edge cases

**Command-Line Script Pattern** — `scripts/score_data.R:1-41`
- Shebang line: `#!/usr/bin/env Rscript` — `scripts/score_data.R:1`
- Console messages with `cat()` to show progress: "Loading...", "Calculating...", "Writing..."
- Summary output at end: `print(summary())` — `scripts/score_data.R:39-40`
- **Phase 4 pattern:** Create `scripts/generate_report.R` with argument parsing (use `commandArgs(trailingOnly = TRUE)` or optparse package)

**Coverage Report Pattern** — `scripts/coverage_report.R`
- Uses `covr::package_coverage()` to generate coverage metrics
- Prints per-file and overall coverage percentages
- Validates 100% target: `if (overall_coverage < 100) stop("Coverage target not met")`
- **Phase 4 usage:** Run after implementing suggestions engine to verify 100% coverage maintained

### Dependencies & Integration Points

**R Package Dependencies (DESCRIPTION:9-22)**
- Imports (required): dplyr, tibble, purrr, shiny, shinydashboard, DT, plotly, ggplot2
- Suggests (optional): testthat, covr, rprojroot, shinytest2
- **Phase 4 additions required:** knitr, rmarkdown (for Quarto rendering) — must add to Imports, not Suggests (per Phase 3 lesson)
- **Optional:** quarto R package (can call Quarto CLI directly via `system()` instead)

**External Dependency: Quarto CLI**
- SPEC.md:76 requires Quarto CLI installed on system (not R package)
- Check command: `quarto --version`
- Installation: https://quarto.org/docs/get-started/ or `brew install quarto` (Mac)
- **Current status:** Not installed on system (verified: `which quarto` returned "not found")
- **Phase 4 requirement:** Document installation in AGENTS.md before implementation, test report rendering after Quarto installed

**Integration: Scored Data as Report Input**
- Report must accept scored CSV as input parameter (SPEC.md:35-36)
- Default input: `data/scored_reps.csv` (generated by `scripts/score_data.R`)
- Alternative input: Exported CSV from Shiny dashboard (includes custom weights reflected in scores)
- **Column schema expected:** 15 columns (11 original + 4 scores: activity_score, conversion_score, revenue_score, score)

**Integration: Quarto Rendering Workflow**
- Quarto template file: `reports/template.qmd` (to be created)
- Rendering command: `quarto render reports/template.qmd --to html --execute-params '{"input_csv": "data/scored_reps.csv"}'`
- Or use R quarto package: `quarto::quarto_render("reports/template.qmd", execute_params = list(input_csv = "..."))`
- Or call CLI via system command: `system2("quarto", args = c("render", "reports/template.qmd", ...))`
- **Output location:** `reports/` directory with timestamped filename (e.g., executive_report_2026-02-17.html)

**Integration: Suggestions in Shiny Dashboard (Optional)**
- If time permits: Add suggestions column to rankings table — `app.R:82`
- Would require: Generate suggestions in reactive scoring step, bind to scored_data, display in DT table
- Add filter dropdown for suggestion_category — similar to existing rep/period filters at `app.R:47-56`
- **Priority:** Report is higher priority (SPEC.md:73-74), only implement if suggestions engine and report complete

### Test Infrastructure

**Test Framework: testthat 3rd edition**
- Configured in DESCRIPTION:23 — `Config/testthat/edition: 3`
- Test runner: `tests/testthat.R` standard entry point
- Command: `Rscript -e "testthat::test_dir('tests/testthat')"`
- Current status: 190 passing tests, 8 skipped (shinytest2 E2E), 0 failures
- Coverage: 100% for all R source files (R/calculate_scores.R, R/dimension_scoring.R, R/generate_sample_data.R, R/normalization.R, R/scoring_utils.R, R/shiny_helpers.R)

**Test Patterns Observed**
- Anti-mock bias: Tests use real implementations, not mocks (per AGENTS.md:150 convention)
- Source all dependencies explicitly at top of test file using `rprojroot::find_root("DESCRIPTION")`
- Example: `tests/testthat/test-calculate_scores.R:1-5` sources 4 dependencies
- Test data uses sample CSV: `read.csv(file.path(rprojroot::find_root("DESCRIPTION"), "data", "sample_reps.csv"))`

**Integration Tests** — `tests/testthat/test-integration.R`
- End-to-end tests verify full pipeline: load sample data → calculate_scores() → validate output structure/ranges
- Example: `tests/testthat/test-integration.R:7-33` validates 80 rows, score ranges 0-100, no NAs/Infs
- Known rep validation: `tests/testthat/test-integration.R:35-50` validates specific rep scores match expectations
- **Phase 4 pattern:** Create integration test for full workflow: load scored CSV → generate_suggestions() → validate suggestion count/structure → render report → verify HTML file exists

**Current Coverage Report Output**
```
Coverage: 100.00%
R/calculate_scores.R: 100.00%
R/dimension_scoring.R: 100.00%
R/generate_sample_data.R: 100.00%
R/normalization.R: 100.00%
R/scoring_utils.R: 100.00%
R/shiny_helpers.R: 100.00%
Overall coverage: 100.0%
```
- **Phase 4 requirement:** Maintain 100% coverage by adding `R/generate_suggestions.R: 100.00%` to this list

**Performance Tests** — `tests/testthat/test-performance.R`
- Validates scoring completes in < 500ms for 1000 rows (SPEC Phase 3 requirement)
- Current performance: 0.014s for 1000 rows, 0.009s for 80 rows (well under target)
- **Phase 4 requirement:** Report generation must complete in < 30 seconds for 1000-row dataset (SPEC.md:78)

**shinytest2 Tests (Currently Skipped)** — `tests/testthat/test-app.R`
- 8 E2E tests for Shiny dashboard: app launch, weight sliders, filters, export, debug mode, CSV upload
- All tests skip with "On CRAN" reason due to test environment settings
- Uses `AppDriver$new()` to launch app in headless browser
- **Phase 4 impact:** Not a blocker — report generation doesn't depend on Shiny tests passing

## Code References

### Scoring Engine (Complete, No Changes Needed)
- `R/calculate_scores.R:60-97` — Main scoring pipeline function
- `R/dimension_scoring.R:18-37` — Activity quality scoring
- `R/dimension_scoring.R:57-76` — Conversion efficiency scoring
- `R/dimension_scoring.R:95-109` — Revenue contribution scoring
- `R/scoring_utils.R:57-66` — Percentile ranking function
- `R/normalization.R:14-22, 38-46, 61-74` — Tenure, territory, quota normalization

### Data Files
- `data/sample_reps.csv` — Original sample data (11 columns, 80 rows)
- `data/scored_reps.csv` — Scored output (15 columns: original + 4 scores)

### Scripts
- `scripts/generate_data.R` — Generates sample_reps.csv
- `scripts/score_data.R` — Generates scored_reps.csv from sample data
- `scripts/coverage_report.R` — Runs test coverage and validates 100% target

### Shiny Dashboard
- `app.R:1-134` — UI definition (dashboardPage with 3 tabs: Rankings, Trends, Upload)
- `app.R:136-427` — Server logic (reactives for scoring, filtering, charting, export)
- `app.R:268-285` — CSV export with timestamp (pattern for report output filename)

### Test Infrastructure
- `tests/testthat.R` — Test runner entry point
- `tests/testthat/test-calculate_scores.R:1-120` — Weight validation and scoring pipeline tests
- `tests/testthat/test-integration.R:7-50` — End-to-end integration tests
- `tests/testthat/test-shiny_helpers.R:4-99` — Validation and normalization helper tests

### Documentation
- `AGENTS.md:136-175` — Coding conventions (tidyverse style, roxygen2, TDD, 100% coverage)
- `AGENTS.md:103-134` — Scoring methodology explanation
- `AGENTS.md:227-275` — Dashboard usage guide (pattern for report generation docs)
- `CLAUDE.md:9-47` — Quick command reference (add report generation command here)
- `README.md:86-115` — Scoring methodology section (pattern for report documentation)

## Open Questions

### Quarto Dependency Management
- **Question:** Use quarto R package or call CLI directly via system()?
- **Context:** SPEC.md:214 mentions quarto R package as optional
- **Options:**
  1. Use quarto R package: `quarto::quarto_render()` — cleaner R interface, easier to pass parameters
  2. Use system() call: `system2("quarto", args = c("render", ...))` — no R package dependency, but less robust error handling
- **Consideration:** quarto R package not currently in DESCRIPTION — adding it would require testing installation
- **Recommendation:** Start with system() approach for MVP, document quarto R package as optional enhancement

### Suggestion Priority Logic
- **Question:** When multiple rules match a rep, which suggestion should be returned?
- **Context:** SPEC.md:56 says "return most critical suggestion per rep (not all matching rules)"
- **Options:**
  1. Low overall score (< 40) takes precedence — indicates systemic issues
  2. First matching rule wins — simplest logic, deterministic
  3. Weighted priority: conversion > activity > revenue — domain-specific
- **Consideration:** No explicit priority guidance in SPEC
- **Recommendation:** Use option 1 (low overall score precedence) — comprehensive coaching for struggling reps is most critical

### Report Rendering Failure Handling
- **Question:** If Quarto CLI not installed, should script error or provide installation instructions?
- **Context:** SPEC.md:129 requires "error message with installation instructions" if Quarto missing
- **Options:**
  1. Check `Sys.which("quarto")` before rendering, stop with install message if empty
  2. Attempt render, catch error, parse error message to detect missing CLI
- **Consideration:** Better UX to check upfront than fail mid-render
- **Recommendation:** Use option 1 — check before render, provide clear installation URL in error message

### Trend Charts for Single Period
- **Question:** How to handle trend analysis when data contains only single period?
- **Context:** SPEC.md:138 mentions "single period → no trend charts (or shows 'insufficient data' message)"
- **Options:**
  1. Omit trend section entirely if < 2 periods
  2. Show trend section with "Insufficient data (minimum 2 periods required)" message
  3. Show single data point on chart with explanatory note
- **Consideration:** Report structure should be consistent regardless of data
- **Recommendation:** Use option 2 — include section with clear message, maintains report structure

### PDF Rendering Without LaTeX
- **Question:** Should PDF generation be optional acceptance criterion?
- **Context:** SPEC.md:96 says "Report renders in PDF format if LaTeX installed (optional, document requirement)"
- **Consideration:** LaTeX not installed on current system, adds setup complexity
- **Recommendation:** Mark PDF rendering as optional, focus on HTML (primary format per SPEC.md:36), document LaTeX requirement in AGENTS.md

### Dashboard Integration Scope
- **Question:** Implement suggestions in Shiny dashboard during Phase 4?
- **Context:** SPEC.md:73-74 marks dashboard integration as optional "if time permits"
- **Context:** REFLECTIONS.md:279-282 reinforces report as higher priority
- **Consideration:** Report is primary deliverable (BRIEF.md:41-45), dashboard enhancement is nice-to-have
- **Recommendation:** Defer dashboard integration — focus vertical slice on suggestions engine + report rendering. Revisit only if report complete with time remaining.

## Directory Structure Notes

**Missing Directories (To Be Created)**
- `reports/` — Directory for Quarto template and generated reports
- `reports/template.qmd` — Quarto report template file (SPEC.md:81)
- `R/generate_suggestions.R` — Suggestions engine implementation (SPEC.md:47-57)
- `tests/testthat/test-generate_suggestions.R` — Test file for suggestions engine

**Existing Structure (No Changes Needed)**
- `R/` — 6 source files (scoring engine complete)
- `data/` — 2 CSV files (sample and scored data)
- `scripts/` — 3 executable scripts (generate_data, score_data, coverage_report)
- `tests/testthat/` — 10 test files (190 passing tests, 100% coverage)
- `docs/phases/phase-1/`, `phase-2/`, `phase-3/` — Prior phase documentation
- `docs/phases/phase-4/` — Current phase (SPEC.md exists, RESEARCH.md in progress)
